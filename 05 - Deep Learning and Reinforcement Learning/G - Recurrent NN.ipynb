{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part g: RNN DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNNs to classify sentiment on textual data\n",
    "For this exercise, we will train a \"vanilla\" RNN to predict the sentiment on reviews.  Our data consists of 25000 training sequences and 25000 test sequences.  The outcome is binary (positive/negative) and both outcomes are equally represented in both the training and the test set.\n",
    "\n",
    "Keras provides a convenient interface to load the data and immediately encode the words into integers (based on the most common words).  This will save us a lot of the drudgery that is usually involved when working with raw text.\n",
    "\n",
    "We will walk through the preparation of the data and the building of an RNN model.  Then it will be your turn to build your own models (and prepare the data how you see fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 30  # maximum length of a sequence - truncate after this\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "## Load in the data.  The function automatically tokenizes the text into distinct integers\n",
    "(x_train, y_train), (x_test, y_test) = imported_data.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "# each sequence represents a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 30)\n",
      "x_test shape: (25000, 30)\n"
     ]
    }
   ],
   "source": [
    "# This pads (or truncates) the sequences so that they are of the maximum length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  219,   141,    35,   221,   956,    54,    13,    16,    11,\n",
       "        2714,    61,   322,   423,    12,    38,    76,    59,  1803,\n",
       "          72,     8, 10508,    23,     5,   967,    12,    38,    85,\n",
       "          62,   358,    99], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[123,:]  #Here's what an example sequence looks like\n",
    "# This is a sentence consists of 30 words, and each number represents a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras layers for (Vanilla) RNNs\n",
    "\n",
    "In this exercise, we will not use pre-trained word vectors.  Rather we will learn an embedding as part of the Neural Network.  This is represented by the Embedding Layer below.\n",
    "\n",
    "### Embedding Layer [Embedding will make similar words have vectors close to each other] \n",
    "`keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n",
    "\n",
    "- This layer maps each integer into a distinct (dense) word vector of length `output_dim`.\n",
    "- Can think of this as learning a word vector embedding \"on the fly\" rather than using an existing mapping (like GloVe) [So it would be specific to our data context]\n",
    "- The `input_dim` should be the size of the vocabulary.\n",
    "- The `input_length` specifies the length of the sequences that the network expects.\n",
    "\n",
    "### SimpleRNN Layer\n",
    "`keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- Kernel_initializer is going to be the weights for the inputs.\n",
    "- recurrent_initializer is going to be the initialized weights for those state layers.\n",
    "- This is the basic RNN, where the output is also fed back as the \"hidden state\" to the next iteration.\n",
    "- The parameter `units` gives the dimensionality of the output (and therefore the hidden state).  Note that typically there will be another layer after the RNN mapping the (RNN) output to the network output.  So we should think of this value as the desired dimensionality of the hidden state and not necessarily the desired output of the network.\n",
    "- Recall that there are two sets of weights, one for the \"recurrent\" phase and the other for the \"kernel\" phase.  These can be configured separately in terms of their initialization, regularization, etc.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape[0])\n",
    "print(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 16:54:26.644203: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## Let's build a RNN\n",
    "\n",
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 50 #we will take the current integers (words) and given their context, we will come up with an embedding where it's going to transfer each integer into an embedded vector thats of dimension 50.\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence and embeds it in a 50-dimensional vector\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:])) # Shape of a one single vector \n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid')) # so we can have an output of 1/0 [positive or negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1000000   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 5)                 280       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,000,286\n",
      "Trainable params: 1,000,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Note that most of the parameters come from the embedding layer\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 14s 16ms/step - loss: 0.6546 - accuracy: 0.6262 - val_loss: 0.5935 - val_accuracy: 0.6967\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 0.5470 - accuracy: 0.7294 - val_loss: 0.5316 - val_accuracy: 0.7318\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 0.4834 - accuracy: 0.7681 - val_loss: 0.4974 - val_accuracy: 0.7526\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.4430 - accuracy: 0.7942 - val_loss: 0.4794 - val_accuracy: 0.7662\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4152 - accuracy: 0.8108 - val_loss: 0.4635 - val_accuracy: 0.7757\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.3951 - accuracy: 0.8229 - val_loss: 0.4534 - val_accuracy: 0.7824\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.3809 - accuracy: 0.8289 - val_loss: 0.4518 - val_accuracy: 0.7856\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3704 - accuracy: 0.8332 - val_loss: 0.4479 - val_accuracy: 0.7876\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3616 - accuracy: 0.8387 - val_loss: 0.4468 - val_accuracy: 0.7902\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3555 - accuracy: 0.8439 - val_loss: 0.4462 - val_accuracy: 0.7917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc837482eb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4462 - accuracy: 0.7917\n",
      "Test score: 0.44624343514442444\n",
      "Test accuracy: 0.7917199730873108\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_rnn.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise, we will illustrate:\n",
    "- Preparing the data to use sequences of length 80 rather than length 30.  Does it improve the performance?\n",
    "- Trying different values of the \"max_features\".  Does this  improve the performance?\n",
    "- Trying smaller and larger sizes of the RNN hidden dimension.  How does it affect the model performance?  How does it affect the run time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 80  # maximum length of a sequence - truncate after this\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imported_data.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 50\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 23s 28ms/step - loss: 0.6090 - accuracy: 0.6609 - val_loss: 0.5167 - val_accuracy: 0.7504\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.4578 - accuracy: 0.7870 - val_loss: 0.5235 - val_accuracy: 0.7324\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.3890 - accuracy: 0.8274 - val_loss: 0.4128 - val_accuracy: 0.8144\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.3476 - accuracy: 0.8509 - val_loss: 0.3953 - val_accuracy: 0.8229\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.3194 - accuracy: 0.8660 - val_loss: 0.3857 - val_accuracy: 0.8294\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.2986 - accuracy: 0.8742 - val_loss: 0.3810 - val_accuracy: 0.8296\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 24s 31ms/step - loss: 0.2831 - accuracy: 0.8816 - val_loss: 0.3669 - val_accuracy: 0.8378\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.2705 - accuracy: 0.8896 - val_loss: 0.3660 - val_accuracy: 0.8384\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.2604 - accuracy: 0.8937 - val_loss: 0.3702 - val_accuracy: 0.8376\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.2519 - accuracy: 0.8967 - val_loss: 0.3722 - val_accuracy: 0.8388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc8398fb7c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 80  # maximum length of a sequence - truncate after this\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imported_data.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 20\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 24s 29ms/step - loss: 0.6503 - accuracy: 0.6179 - val_loss: 0.5783 - val_accuracy: 0.7028\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.5200 - accuracy: 0.7473 - val_loss: 0.5012 - val_accuracy: 0.7572\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.4602 - accuracy: 0.7812 - val_loss: 0.4686 - val_accuracy: 0.7712\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.4186 - accuracy: 0.8103 - val_loss: 0.4761 - val_accuracy: 0.7809\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.3853 - accuracy: 0.8307 - val_loss: 0.4210 - val_accuracy: 0.8069\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 23s 30ms/step - loss: 0.3643 - accuracy: 0.8411 - val_loss: 0.4074 - val_accuracy: 0.8115\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.3505 - accuracy: 0.8481 - val_loss: 0.4014 - val_accuracy: 0.8148\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.3394 - accuracy: 0.8533 - val_loss: 0.3933 - val_accuracy: 0.8208\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.3298 - accuracy: 0.8584 - val_loss: 0.3989 - val_accuracy: 0.8210\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 23s 30ms/step - loss: 0.3214 - accuracy: 0.8637 - val_loss: 0.3830 - val_accuracy: 0.8276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc83b1a9670>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 22s 29ms/step - loss: 0.3150 - accuracy: 0.8649 - val_loss: 0.3802 - val_accuracy: 0.8291\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.3086 - accuracy: 0.8707 - val_loss: 0.3734 - val_accuracy: 0.8325\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.3026 - accuracy: 0.8724 - val_loss: 0.3693 - val_accuracy: 0.8340\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 23s 30ms/step - loss: 0.2982 - accuracy: 0.8740 - val_loss: 0.3711 - val_accuracy: 0.8339\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 23s 30ms/step - loss: 0.2939 - accuracy: 0.8756 - val_loss: 0.3674 - val_accuracy: 0.8365\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 23s 30ms/step - loss: 0.2897 - accuracy: 0.8769 - val_loss: 0.3628 - val_accuracy: 0.8389\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 24s 31ms/step - loss: 0.2865 - accuracy: 0.8794 - val_loss: 0.3639 - val_accuracy: 0.8384\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.2835 - accuracy: 0.8812 - val_loss: 0.3620 - val_accuracy: 0.8401\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 25s 31ms/step - loss: 0.2807 - accuracy: 0.8822 - val_loss: 0.3637 - val_accuracy: 0.8401\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.2789 - accuracy: 0.8833 - val_loss: 0.3663 - val_accuracy: 0.8389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc819806df0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out of curiosity, run for 10 more epochs\n",
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying a Different Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 40  # maximum length of a sequence - truncate after this\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imported_data.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hidden_dim = 8\n",
    "word_embedding_dim = 25\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 [==============================] - 18s 21ms/step - loss: 0.6650 - accuracy: 0.6013 - val_loss: 0.6243 - val_accuracy: 0.6560\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.5890 - accuracy: 0.6891 - val_loss: 0.5616 - val_accuracy: 0.7115\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.5453 - accuracy: 0.7224 - val_loss: 0.5463 - val_accuracy: 0.7178\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.5162 - accuracy: 0.7411 - val_loss: 0.5070 - val_accuracy: 0.7536\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.4944 - accuracy: 0.7589 - val_loss: 0.4882 - val_accuracy: 0.7652\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.4777 - accuracy: 0.7708 - val_loss: 0.4751 - val_accuracy: 0.7700\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.4658 - accuracy: 0.7778 - val_loss: 0.4767 - val_accuracy: 0.7665\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.4583 - accuracy: 0.7800 - val_loss: 0.4631 - val_accuracy: 0.7776\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.4533 - accuracy: 0.7830 - val_loss: 0.4604 - val_accuracy: 0.7777\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.4501 - accuracy: 0.7841 - val_loss: 0.4609 - val_accuracy: 0.7782\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4468 - accuracy: 0.7875 - val_loss: 0.4576 - val_accuracy: 0.7811\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.4449 - accuracy: 0.7882 - val_loss: 0.4553 - val_accuracy: 0.7828\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 0.4425 - accuracy: 0.7908 - val_loss: 0.4571 - val_accuracy: 0.7790\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.4411 - accuracy: 0.7914 - val_loss: 0.4540 - val_accuracy: 0.7836\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.4400 - accuracy: 0.7918 - val_loss: 0.4532 - val_accuracy: 0.7852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc8173d9e80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 0.4532 - accuracy: 0.7852\n",
      "Test score: 0.4532153904438019\n",
      "Test accuracy: 0.7851600050926208\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_rnn.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
