{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part e: CNN DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN to classify images in the CIFAR-10 Dataset\n",
    "\n",
    "We will work with the CIFAR-10 Dataset.  This is a well-known dataset for image classification, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The 10 classes are:\n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> airplane\n",
    "<li>  automobile\n",
    "<li> bird\n",
    "<li>  cat\n",
    "<li> deer\n",
    "<li> dog\n",
    "<li>  frog\n",
    "<li>  horse\n",
    "<li>  ship\n",
    "<li>  truck\n",
    "</ol>\n",
    "\n",
    "For details about CIFAR-10 see:\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "For a compilation of published performance results on CIFAR 10, see:\n",
    "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
    "\n",
    "---\n",
    "\n",
    "### Building Convolutional Neural Nets\n",
    "\n",
    "In this exercise we will build and train our first convolutional neural networks.  In the first part, we walk through the different layers and how they are configured.  In the second part, you will build your own model, train it, and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "#from tensorflow.keras.datasets import cifar10\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential # For model initiation\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten \n",
    "# Dense to create fully connected layer # Drop out for regularization\n",
    "# Flatten is important when we want to move from convolutional layer to dense layer \n",
    "from keras.layers import Conv2D, MaxPooling2D # To build convolutional layers and pooling layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 14s 0us/step\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Each image is a 32 x 32 x 3 numpy array\n",
    "x_train[444].shape # image number 444\n",
    "# It is 32 times 32 matrices with 9 elements per matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's look at one of the images\n",
    "\n",
    "print(y_train[444]) # print the class of the image\n",
    "plt.imshow(x_train[444]); # print the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's look at another image\n",
    "\n",
    "print(y_train[144]) # class 2 is bird.\n",
    "plt.imshow(x_train[144]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 # defined how many classes we have \n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes) # encoding to pythonic array\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes) # encoding to pythonic array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now instead of classes described by an integer between 0-9 we have a vector with a 1 in the (Pythonic) 9th position\n",
    "y_train[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[144] # should give us a 1 in the 2nd pythonic position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, let's make everything float and scale\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 # Min-max scaling.\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train) # it stays an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Layers for CNNs\n",
    "- Previously we built Neural Networks using primarily the Dense, Activation and Dropout Layers.\n",
    "\n",
    "- Here we will describe how to use some of the CNN-specific layers provided by Keras\n",
    "\n",
    "### Conv2D\n",
    "\n",
    "```python\n",
    "keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "```\n",
    "\n",
    "A few parameters explained:\n",
    "- `filters`: the number of filter used per location.  In other words, the depth of the output. [Number of kernels]\n",
    "- `kernel_size`: an (x,y) tuple giving the height and width of the kernel to be used\n",
    "- `strides`: and (x,y) tuple giving the stride in each dimension.  Default is `(1,1)`\n",
    "- `input_shape`: required only for the first layer\n",
    "- `padding`: if padding means valid then it means that we are not having any paddings.\n",
    "\n",
    "Note, the size of the output will be determined by the kernel_size, strides\n",
    "\n",
    "### MaxPooling2D\n",
    "`keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n",
    "\n",
    "- `pool_size`: the (x,y) size of the grid to be pooled.\n",
    "- `strides`: Assumed to be the `pool_size` unless otherwise specified\n",
    "\n",
    "### Flatten\n",
    "Turns its input into a one-dimensional vector (per instance).  Usually used when transitioning between convolutional layers and fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "## First CNN\n",
    "Below we will build our first CNN.  For demonstration purposes (so that it will train quickly) it is not very deep and has relatively few parameters.  We use strides of 2 in the first two convolutional layers which quickly reduces the dimensions of the output.  After a MaxPooling layer, we flatten, and then have a single fully connected layer before our final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        2432      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 6, 6, 32)          25632     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 14:11:20.943476: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 3, 3, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 3, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               147968    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,162\n",
      "Trainable params: 181,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same', # We are adding paddings\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2))) # take the max of each distinct batck and reduce it\n",
    "model_1.add(Dropout(0.25)) # Regularizarion\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1.add(Flatten()) # moving from 3 dimensional object to 1 dimensional object.\n",
    "model_1.add(Dense(512)) \n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes)) # adding final dense layer, so our output is equal to number of classes we have.\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 181K parameters, even though this is a \"small\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.7376 - accuracy: 0.3643 - val_loss: 1.4504 - val_accuracy: 0.4871\n",
      "Epoch 2/15\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.4504 - accuracy: 0.4784 - val_loss: 1.3511 - val_accuracy: 0.5223\n",
      "Epoch 3/15\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.3505 - accuracy: 0.5141 - val_loss: 1.2858 - val_accuracy: 0.5401\n",
      "Epoch 4/15\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.2821 - accuracy: 0.5433 - val_loss: 1.2314 - val_accuracy: 0.5621\n",
      "Epoch 5/15\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.2345 - accuracy: 0.5611 - val_loss: 1.1964 - val_accuracy: 0.5740\n",
      "Epoch 6/15\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.2023 - accuracy: 0.5775 - val_loss: 1.1513 - val_accuracy: 0.5913\n",
      "Epoch 7/15\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.1727 - accuracy: 0.5884 - val_loss: 1.1002 - val_accuracy: 0.6138\n",
      "Epoch 8/15\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.1595 - accuracy: 0.5925 - val_loss: 1.1125 - val_accuracy: 0.6164\n",
      "Epoch 9/15\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.1449 - accuracy: 0.6007 - val_loss: 1.1340 - val_accuracy: 0.6017\n",
      "Epoch 10/15\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.1326 - accuracy: 0.6045 - val_loss: 1.1215 - val_accuracy: 0.6080\n",
      "Epoch 11/15\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.1223 - accuracy: 0.6098 - val_loss: 1.1208 - val_accuracy: 0.6152\n",
      "Epoch 12/15\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.1220 - accuracy: 0.6110 - val_loss: 1.1017 - val_accuracy: 0.6265\n",
      "Epoch 13/15\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.1146 - accuracy: 0.6151 - val_loss: 1.1085 - val_accuracy: 0.6278\n",
      "Epoch 14/15\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.1167 - accuracy: 0.6191 - val_loss: 1.1823 - val_accuracy: 0.5995\n",
      "Epoch 15/15\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.1169 - accuracy: 0.6167 - val_loss: 1.1131 - val_accuracy: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f88e43e79a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compiling and fitting\n",
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_1.compile(loss='categorical_crossentropy', # categorical cross entropy because it is a multiclass classification\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "              batch_size=batch_size, # how many rows [image] per iteration\n",
    "              epochs=15,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step\n",
      "313/313 [==============================] - 2s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 1, 0, ..., 5, 1, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the probability:\n",
    "model_1_proba = model_1.predict(x_test)\n",
    "# Predict the class\n",
    "model_1_class = np.argmax(model_1.predict(x_test), axis=1)\n",
    "model_1_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to perform accuracy metrics, let us convert y_test into integers\n",
    "y_test_int = np.argmax(y_test, axis = 1)\n",
    "y_test_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6104"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_int, model_1_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Our previous model had the structure:\n",
    "\n",
    "Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "(with appropriate activation functions and dropouts)\n",
    "\n",
    "1. Build a more complicated model with the following pattern:\n",
    "- Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "- Use strides of 1 for all convolutional layers.\n",
    "\n",
    "2. How many parameters does your model have?  How does that compare to the previous model?\n",
    "\n",
    "3. Train it for 5 epochs.  What do you notice about the training time, loss and accuracy numbers (on both the training and validation sets)?\n",
    "\n",
    "5. Try different structures and run times, and see how accurate your model can be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Conv2D(32, (3, 3), padding='same', # first time with padding\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(32, (3, 3))) # second time without padding\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(64, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(512))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Check number of parameters\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt_2,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 175s 111ms/step - loss: 1.5693 - accuracy: 0.4280 - val_loss: 1.3162 - val_accuracy: 0.5388\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 1.1912 - accuracy: 0.5779 - val_loss: 1.1966 - val_accuracy: 0.5831\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 255s 163ms/step - loss: 1.0197 - accuracy: 0.6436 - val_loss: 0.8806 - val_accuracy: 0.6906\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.9187 - accuracy: 0.6800 - val_loss: 0.9601 - val_accuracy: 0.6646\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 209s 133ms/step - loss: 0.8630 - accuracy: 0.7026 - val_loss: 0.8269 - val_accuracy: 0.7149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f88803a8a60>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 23ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the probability:\n",
    "model_2_proba = model_2.predict(x_test)\n",
    "# Predict the class\n",
    "model_2_class = np.argmax(model_2.predict(x_test),axis = 1)\n",
    "model_2_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test2_int = np.argmax(y_test, axis = 1)\n",
    "y_test2_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7149"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test2_int, model_2_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
